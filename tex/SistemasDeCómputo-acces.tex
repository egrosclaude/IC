\documentclass[spanish,A4,]{article}
\usepackage{sans}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\usepackage{longtable,booktabs}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{0}
\ifxetex
  \usepackage{polyglossia}
  \setmainlanguage{spanish}
\else
  \usepackage[spanish]{babel}
\fi

\title{Sistemas de cómputo}

\begin{document}
\maketitle

\section{Sistemas de Cómputo}\label{sistemas-de-cuxf3mputo}

Un sistema de cómputo es un conjunto de elementos electrónicos
organizados para permitir el procesamiento de información. A lo largo
del tiempo los sistemas de cómputo han evolucionado notablemente, y en
este material vamos a ver las cosas más importantes que han ocurrido
durante esta evolución.

\subsection{Evolución de los sistemas de
cómputo}\label{evoluciuxf3n-de-los-sistemas-de-cuxf3mputo}

Los sistemas de cómputo constituyen una industria, y como en toda
industria, existe la competencia entre los fabricantes, que buscan
obtener más mercado mejorando las características de sus productos. En
ninguna otra industria como en la computación es tan notable el
crecimiento de las capacidades de los sistemas de cómputo y,
simultáneamente, la reducción de costos y tamaños físicos de los
componentes.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Las sucesivas generaciones de sistemas de cómputo han ido creciendo en
  la velocidad de procesamiento y capacidad de memoria, así como en las
  capacidades de los discos y otras unidades de almacenamiento.
\item
  Al mismo tiempo, se ha reducido notablemente el tamaño del sistema
  como un todo. Esto último se ha logrado a través de sucesivos avances
  en la \textbf{integración} de los componentes.
\item
  La mayor integración ha facilitado la \textbf{economía de escala},
  manteniendo o reduciendo en general los costos de producción.
\end{itemize}

Sin embargo, los diferentes componentes se producen con procesos de
fabricación diferentes y que incorporan avances cada uno a su ritmo, por
lo cual no siempre más rápido o más pequeño es mejor, sino que lo más
importante es que las partes del sistema funcionen en armonía. De lo
contrario, pueden presentarse desbalances que impidan que el sistema
funcione en forma óptima.

\subsubsection{Antecedentes
históricos}\label{antecedentes-histuxf3ricos}

En la antigüedad se crearon los que podríamos llamar sistemas de cómputo
primitivos, aunque ingeniosos. Aquí citamos unos pocos ejemplos, como el
ábaco chino; los quipus incas; los dispositivos de cálculo analógicos
aparecidos en diferentes civilizaciones, como los que permitían calcular
la torsión de los tensores de las catapultas romanas; el misterioso
Mecanismo de Anticitera, un aparato astronómico encontrado entre los
restos de un naufragio griego.

Más cerca de nuestros días, se crearon artefactos parecidos a las
calculadoras y computadoras actuales, pero con la tecnología disponible
en esos momentos, lo que lógicamente los hacían incómodos, o pobres en
resultados, en comparación con las herramientas electrónicas de hoy.

Supuestamente construido por científicos griegos, el instrumento
conocido como
\href{https://es.m.wikipedia.org/wiki/Mecanismo_de_Anticitera}{Mecanismo
de Anticitera} data de entre los años 150 a. C. y 100 a. C., o, según
una observación reciente, del año 200 a. C.

Inventada en 1642 por el filósofo y matemático francés Blaise Pascal
(1623-1662), \href{https://es.m.wikipedia.org/wiki/Pascalina}{la
Pascalina} permitía sumar y restar dos números de manera directa y hacer
la multiplicación y división por repetición.

\subsubsection{La Tabuladora de
Hollerith}\label{la-tabuladora-de-hollerith}

Entre estos proto-sistemas de computación es especialmente notable, por
varios motivos, la máquina de Hollerith. Herman Hollerith trabajaba para
la Oficina de Inmigración de EEUU hacia fines del siglo XIX, en momentos
en que se formó una gran corriente inmigratoria desde Europa. La gran
cantidad de personas que llegaban a radicarse, nunca vista antes, hizo
desbordar el sistema de información nacional. Los responsables del censo
poblacional se encontraban con un gran problema.

Mucho antes de Hollerith, un tapicero francés, Jacquard, había ideado un
telar que se configuraba usando tarjetas perforadas. Alimentado con
estas tarjetas, el telar creaba automáticamente el dibujo deseado.
Inspirado en el telar de Jacquard, Hollerith creó un sistema de cómputo
automático \textbf{electromecánico} basado en tarjetas perforadas. Cada
tarjeta representaba a un individuo. La tarjeta se dividía en campos que
representaban los atributos o características personales del individuo
(nacionalidad, fecha de nacimiento, sexo, estado civil, etc). Al llegar
un individuo, el oficial de inmigración le presentaba un cuestionario y
codificaba sus respuestas con una perforación en cierto lugar de cada
campo.

La Tabuladora de Hollerith era un dispositivo que contabilizaba
perforaciones en esas tarjetas. Podía ser programada para contar la
cantidad de individuos por nacionalidad, por edad, por sexo, etc., o por
varios de estos atributos simultáneamente. De esa manera el censo
nacional pudo lograrse en muchísimo menos tiempo que con los anteriores
métodos manuales.

La máquina de Hollerith es especialmente interesante porque sienta las
bases del cálculo digital como se conocerá en los años siguientes (de
hecho, las tarjetas perforadas siguieron utilizándose hasta muchos años
después como medio de entrada, para codificar programas y datos), porque
demostró el poder del cómputo automático con una aplicación concreta e
importante, y porque, tomando su invento como punto de partida,
Hollerith formó una importante empresa de computación que tuvo gran
influencia en el desarrollo de la tecnología del siglo XX.

\subsection{Primera generación}\label{primera-generaciuxf3n}

Las primeras computadoras electrónicas usaban \textbf{bulbos, tubos de
vacío, o válvulas}, como interruptores, implementando dispositivos que
realizaban operaciones aritméticas y lógicas.

Dado el momento histórico en el cual aparecieron estos equipos, los
objetivos con los cuales se creaban eran, con frecuencia, los usos
militares. Las máquinas de esta generación eran grandes instalaciones
que ocupaban una habitación, y sus miles de válvulas disipaban una gran
cantidad de calor, que debía combatirse con sistemas de aire
acondicionado.

El \textbf{ENIAC} es un claro representante de esta clase de máquinas.
Pesaba 30 toneladas, y ocupaba un recinto de 140 m². Era capaz de
ejecutar 5000 operaciones de suma por segundo. El ENIAC usaba 18000
válvulas de vacío: cada dos días, en promedio, una de ellas fallaba, y
debía ser reemplazada con un procedimiento que llevaba quince minutos.

El ENIAC no era una máquina de Von Neumann porque su programa no residía
en memoria, sino que la computadora se programaba con un intricado
sistema de interruptores manuales. Entre las máquinas de esta generación
se encuentra la primera computadora de programa almacenado según el
modelo de Von Neumann. Fue el IAS (siglas de \textbf{Institute for
Advanced Study}), que usaba 1500 tubos de vacío y tenía 5 kB de memoria.

\subsubsection{El tubo de vacío}\label{el-tubo-de-vacuxedo}

El \textbf{tubo de vacío o válvula termoiónica} fue patentado por Edison
y fue sucesivamente modificado para diferentes usos en electrónica hasta
llegar a ser usado en las computadoras de la primera generación. Una de
sus variedades, el \textbf{triodo}, tiene tres electrodos o terminales
conectados al resto del circuito, llamados \textbf{cátodo, ánodo y
rejilla o grilla de control}. En éstos, la corriente eléctrica se dirige
siempre desde el cátodo al ánodo, pero únicamente circula cuando existe
una determinada carga negativa en la grilla, que funciona como un
interruptor.

De esta manera se puede controlar el flujo de corriente por un circuito
y construir dispositivos que implementen funciones lógicas. Así, dos
válvulas de este tipo, conectadas en serie, simulan una función lógica
de conjunción o \textbf{AND}; dos válvulas conectadas en paralelo,
simulan una disyunción u \textbf{OR}, etc. Con válvulas termoiónicas es
posible además crear un dispositivo que mantenga permanentemente un
cierto estado eléctrico, y que por lo tanto \textbf{puede almacenar un
bit de información}.

La grilla de las válvulas necesita alcanzar una alta temperatura para
poder gobernar el flujo de electrones. De ahí que el consumo de
electricidad fuera altísimo y su funcionamiento sumamente lento. Unido
esto a una alta tasa de fallos, las válvulas fueron rápidamente
abandonadas en favor de una tecnología más conveniente, el
\textbf{transistor}.

\subsubsection{Memorias de núcleos}\label{memorias-de-nuxfacleos}

Las primeras implementaciones de la memoria principal (memorias de
núcleos o \textbf{core memories}) fueron realizadas con pequeños anillos
metálicos atravesados por alambres. El flujo eléctrico que conducían
estos alambres magnetizaba en forma estable los anillos, que almacenaban
un bit de información cada uno. El sistema de memoria podía leer, más
tarde, la \textbf{polaridad} magnética de cada anillo, y así se
recuperaba el valor binario que había sido almacenado en ese bit.

La tecnología de las memorias ha evolucionado espectacularmente desde la
creación de las primitivas memorias de núcleos. Hoy, cincuenta años
después, existen memorias de bajo costo, menor tamaño, mayor velocidad,
y capacidad millones de veces superior. Una memoria Micro-SD de hoy, por
ejemplo, puede alojar 32 GiB de información en una centésima parte del
espacio ocupado por un 1 Kib de memoria de núcleos.

\subsubsection{ENIAC}\label{eniac}

Presentado en 1946, \textbf{ENIAC} es reconocido como el primer
computador digital, completamente electrónico, de propósito general.
Usaba números representados en base 10. Tenía una capacidad de memoria
de 1000 bits donde podía almacenar unos veinte números decimales de diez
dígitos.

El ENIAC fue propuesto para cómputos de trayectoria de proyectiles,
aplicación en la que logró reducir el tiempo de cómputo de una tabla de
datos, de 20 horas a 30 segundos. Sin embargo, la guerra terminó antes
de que pudiera ser realmente aplicado, por lo que se lo destinó a otros
usos. Por este hecho, llamó la atención sobre la capacidad de las
computadoras de ser destinadas a propósitos generales, en lugar de las
máquinas de programa cableado que existían hasta entonces y que eran
preparadas específicamente para una única tarea.

Pero más notable todavía es que cincuenta años y varias generaciones de
computadoras después, en 1997, el proyecto \textbf{ENIAC-on-a-chip}
construyó un circuito integrado completamente equivalente al
ENIAC\ldots{} del tamaño de una moneda (y habrían cabido muchos más en
el mismo espacio).

\subsubsection{Clementina}\label{clementina}

¿Qué pasaba en nuestro país durante estas épocas? La actividad de la
computación aquí no había comenzado. Recién a principios de los años 60
la universidad argentina decidió hacer una importante inversión, que fue
la compra de una computadora de primera generación, bautizada aquí
\textbf{Clementina}. El video adjunto cuenta interesantes detalles
técnicos de la computadora, muestra cómo eran las personalidades
involucradas por ese entonces en el proyecto científico y tecnológico
argentino, y explica el contexto histórico en el que fue iniciado (y,
lamentablemente, truncado) ese proyecto.

\subsection{Segunda generación}\label{segunda-generaciuxf3n}

\subsubsection{Semiconductores}\label{semiconductores}

En 1948 los físicos habían descubierto que combinando, en ciertas
proporciones, elementos que eran vecinos en la Tabla Periódica, se
creaban nuevos materiales con un desbalance de electrones; y que de esta
manera se podía controlar el sentido de las corrientes eléctricas que
atravesaban esos materiales. Así fue inventado un componente electrónico
revolucionario, el \textbf{transistor}, que era básicamente un
\textbf{triodo de estado sólido}, es decir, podía cumplir el mismo papel
en un circuito que la válvula termoiónica de tres electrodos, pero era
construido de una forma completamente diferente.

Esto significa que las mismas funciones lógicas de los interruptores,
que en las computadoras de primera generación eran cumplidas por las
válvulas termoiónicas, podían ser resueltas con dispositivos mucho más
pequeños, de mucho menor consumo, con tiempos de reacción mucho menores
y mucho más confiables. El impacto tecnológico y económico de este
avance fue importantísimo y la computación ``despegó''. Fue posible
aumentar la complejidad de las funciones, creando CPUs mucho más
poderosas.

Decimos que esta segunda generación de computadoras fue construida con
dispositivos \textbf{discretos}, es decir, separados, para distinguirla
de la generación siguiente, donde esos dispositivos fueron
\textbf{integrados}.

\subsubsection{El transistor}\label{el-transistor}

El \textbf{transistor} consiste en tres pequeñas piezas, puestas en
contacto entre sí, hechas de materiales especialmente preparados. Estas
piezas consisten, cada una, de un núcleo de un cierto elemento,
conteniendo impurezas de un segundo elemento diferente.

Si se contamina, en forma controlada, un núcleo principal de germanio o
silicio, con una pequeña proporción de impurezas de boro, aluminio,
arsénico, o galio, el material resultante tiene propiedades de
conductividad especiales.

Según la combinación de los elementos, el material será de tipo
\textbf{P} (atrae electrones de materiales cercanos) o de tipo
\textbf{N} (emite electrones a los materiales cercanos). Disponiendo en
capas alternadas tres pequeños bloques de materiales de tipos P y N se
construyen dispositivos de tipo \textbf{PNP} o de tipo \textbf{NPN}. La
capa central (llamada la \textbf{base} del transistor) actúa como la
grilla de las válvulas: controla el paso de corriente a través de las
otras dos capas (llamadas \textbf{colector} y \textbf{emisor} del
transistor).

Gracias a estas propiedades el transistor funciona como un interruptor,
con el cual se pueden implementar todas las funciones lógicas necesarias
en los circuitos de la CPU y otros componentes de las computadoras. El
transistor resulta así un reemplazo eficaz de las antiguas válvulas
termoiónicas.

\subsection{Del transistor a la
memoria}\label{del-transistor-a-la-memoria}

Un transistor permite el pasaje de corriente entre \textbf{colector y
emisor}, sólo cuando recibe corriente la \textbf{base}. De esta manera
opera como un conmutador o switch.

\subsubsection{Compuertas}\label{compuertas}

Conectando varios transistores adecuadamente es posible crear
\textbf{compuertas}, que son circuitos elementales que implementan
funciones lógicas. Por ejemplo, una \textbf{Compuerta NOR} implementa
una función lógica que es la combinación de otras dos aun más simples:
\textbf{NOT} y \textbf{OR}.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  NOT es una función lógica de una entrada y una salida. El valor lógico
  de salida de NOT es el opuesto de su entrada. Es decir, la salida será
  1 cuando la entrada sea 0, y viceversa.
\item
  OR es una función lógica de dos entradas y una salida. Su valor lógico
  es la disyunción de ambas entradas. Es decir, el valor lógico de la
  salida será 0 sólo si ambas entradas son 0.
\item
  La salida de la función NOR, que es la combinación de ambas
  anteriores, será 1 sólo cuando ambas entradas sean 0.
\end{itemize}

\subsubsection{Flip-Flop}\label{flip-flop}

Un Flip-Flop es un circuito capaz de memorizar un valor lógico, por lo
cual implementa una \textbf{memoria de un bit}. Al poder almacenar dos
posibles valores, 0 o 1, se dice que es un circuito \textbf{biestable}.

Se construye con compuertas (NOR, NAND, XOR\ldots{}) dispuestas de
manera que la salida de cada una realimente la entrada de la otra. Las
dos salidas de un Flip-Flop son, cada una, la inversa lógica de la otra.
Normalmente se utiliza una de ellas.

Las entradas de un Flip-Flop de tipo RS se llaman \textbf{Reset y Set}.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Cuando el procesador necesita almacenar un bit 0, activa la entrada
  Reset. El valor lógico de salida queda permanentemente a 0.
\item
  Cuando se necesita almacenar un 1, se activa la entrada Set, y el
  valor lógico de salida queda establecido en 1.
\end{itemize}

Un \textbf{byte de memoria} puede implementarse como un vector de
\textbf{ocho Flip-Flops}. Un Kibibyte de memoria comprenderá entonces
$2^{10}$ de estos vectores. Las tecnologías modernas de integración en
gran escala permiten concentrar millones de estos componentes en un
espacio muy reducido.

\subsubsection{Minicomputadoras}\label{minicomputadoras}

Los transistores abarataron las computadoras y redujeron su tamaño.
Algunas de las desarrolladas en esta época recibieron el nombre de
\textbf{minicomputadoras}.

El PDP-1 fue uno de los primeros computadores que pudieron ser accedidos
masivamente por los estudiantes de computación. Tenía un \textbf{sistema
de tiempo compartido (time-sharing)} que hacía posible la utilización de
la máquina por varios usuarios a la vez. Tenía 144 KB de memoria
principal y ejecutaba 100.000 instrucciones por segundo.

\subsection{Tercera generación}\label{tercera-generaciuxf3n}

A mediados de los 60 se desarrollaron los \textbf{circuitos integrados o
microchips}, que empaquetaban una gran cantidad de transistores en un
solo componente, con importantes mejoras en el aspecto funcional y en la
economía de la producción de computadoras. Aparecieron computadoras más
baratas que llegaron a empresas y establecimientos educativos más
pequeños, popularizándose el uso de la computación.

Los fabricantes comenzaron a producir familias de computadoras
\textbf{compatibles}, comenzando con el System/360 de IBM. Los productos
de la misma familia utilizaban el mismo lenguaje ensamblador, lo que
permitía la portabilidad de los programas entre diferentes computadoras.
El usuario podía hacer crecer su infraestructura de cómputo sin perder
la inversión hecha en software.

También aparecieron las primeras \textbf{supercomputadoras}, como el
Cray-1, en 1976, que ejecutaba 160 millones de instrucciones por segundo
y tenía 8 MiB de memoria principal.

El \textbf{microprocesador} desarrollado por Intel reunió la mayor parte
de las funciones de las computadoras en un solo microchip. La existencia
del microprocesador favoreció la creación de una industria de las
computadoras personales. En 1982 IBM propuso el PC (Personal Computer),
un \textbf{computador personal o microcomputador} del cual descienden la
mayoría de las computadoras domésticas y de oficina que se usan hoy. Al
contrario que las computadoras de hasta entonces, construidas con
procedimientos y componentes propios del fabricante, y a veces secretos,
la \textbf{arquitectura abierta} del PC utilizaba componentes existentes
y conocidos, y estaba públicamente documentada; de manera que otras
empresas podían libremente fabricar componentes compatibles con esta
computadora.

\subsubsection{Circuitos integrados}\label{circuitos-integrados}

Los \textbf{circuitos integrados} fueron el resultado de un proceso de
fabricación completamente nuevo llamado \textbf{fotomicrolitografía}.
Los ingenieros preparaban un diagrama del circuito deseado, con sus
transistores, conexiones y demás componentes, y el diagrama se reducía
por medios ópticos hasta un tamaño casi microscópico. La imagen
resultante se grababa sobre un sustrato de silicio.

Repitiendo el procedimiento con diferentes diagramas, sobre sucesivas
capas de materiales semiconductores y aislantes, se lograba la
miniaturización de un circuito completo con miles de transistores en un
espacio muy reducido. Los modernos circuitos integrados reúnen miles de
millones de transistores en menos de un centímetro cúbico de volumen.

\subsubsection{Microprocesador}\label{microprocesador}

Un programador utiliza un microprocesador a través de su
\textbf{conjunto de instrucciones} (aritméticas, de transferencia, de
salto, etc.). Por otro lado, el microprocesador tiene un cierto
comportamiento, que el programador necesita conocer: qué papeles cumplen
sus registros, qué efecto tienen las instrucciones sobre esos registros,
qué modos de acceder a los datos son posibles, etc. Este comportamiento
puede llamarse un \textbf{modelo de programación} del microprocesador.

Este conjunto de instrucciones y ese modelo de programación, reunidos,
son la \textbf{arquitectura del conjunto de instrucciones} del
microprocesador, abreviadas \textbf{ISA} por \textbf{Instruction Set
Architecture}.

Este ISA ha sido implementado de alguna forma en el microprocesador, es
decir, los ingenieros han definido un cierto circuito formado por
transistores, compuertas y otros componentes, que hace que el
microprocesador funcione de esta manera. Pero a medida que avanzan las
tecnologías disponibles para la fabricación de los procesadores,
aparecen nuevas formas de implementar ese funcionamiento. Un nuevo
modelo de microprocesador podría tener mayores capacidades, sin cambiar
el ISA. Por ejemplo, podría tener más de una ALU para realizar los
cómputos más rápidamente, en forma paralela; pero manteniendo el mismo
conjunto de instrucciones y el mismo modelo de programación de antes.

Es conveniente que estos cambios queden invisibles al programador,
porque así él seguirá programando de la misma manera y seguirá corriendo
sus programas sin necesidad de modificarlos.

La forma de implementar una arquitectura es llamada la
\textbf{microarquitectura} del microprocesador. Una familia de
microprocesadores puede evolucionar con cambios invisibles, cambiando su
microarquitectura, sin cambiar la \textbf{arquitectura} y sin romper la
compatibilidad con los productos anteriores.

Una familia de microprocesadores también puede ampliar su conjunto de
instrucciones, pero manteniendo intactas todas las de los productos
anteriores. Esta forma de compatibilidad se llama a veces
retrocompatibilidad o \textbf{compatibilidad hacia atrás}.

\subsection{Cuarta generación}\label{cuarta-generaciuxf3n}

Gracias a nuevos procesos de fabricación de circuitos integrados, se
logró cada vez mayor miniaturización de componentes.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  SSI: de 10 a 100 componentes por chip
\item
  MSI: de 100 a 1.000
\item
  LSI: de 1.000 a 10.000
\item
  VLSI: de 10.000 a 100.000
\item
  ULSI: más de 100.000 (típicamente, millones de componentes por chip)
\end{itemize}

Ya en 1965, Gordon Moore había enunciado la observación de que, cada año
y medio, los fabricantes de computadoras ponían el doble de transistores
en un procesador que en el modelo anterior. Unido a que las velocidades
de reloj aumentaban casi en la misma proporción, este hecho daba una
curva de poder de procesamiento claramente creciente año tras año.

Esta mejora en el procesamiento era lo que mantenía en crecimiento a la
industria del hardware, al generar constantemente nueva demanda del
mercado. Renovando su equipamiento cada tres años, un usuario duplicaba
su capacidad de procesamiento. Esta tendencia continuó hasta entrado el
siglo XXI. Sin embargo, los procesadores pronto encontraron obstáculos
que forzaron a un cambio de paso.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  Los componentes se acercaban entre sí cada vez más
\item
  Las señales eléctricas entre los componentes cada vez tardaban menos
  en llegar
\item
  Los ciclos de reloj debían acortarse para reaccionar a tiempo ante la
  llegada de las señales
\item
  Más ciclos de reloj por segundo implicaban mayor consumo de energía
\item
  Debido a la mayor energía consumida, el calor disipado era cada vez
  mayor, agravado por el hecho de que los componentes estaban cada vez
  más cerca
\item
  Los diseñadores debieron enfrentar este problema desde múltiples
  ángulos: disminuyendo la velocidad de reloj; dividiendo los circuitos
  en \textbf{núcleos} o \textbf{cores} que pueden funcionar
  independientemente, y agregando a los microchips funciones de
  monitoreo capaces de desactivar parte de los circuitos mientras no se
  utilizaban
\item
  Esta nueva etapa de los sistemas de cómputo suele llamarse la
  \textbf{Era del Multicore}.
\end{itemize}

Una consecuencia importante de la Era del Multicore es que la mejora en
velocidad de ejecución ya no se logra simplemente comprando hardware
nuevo; sino que los programas deben ser especialmente escritos para
aprovechar las características de \textbf{procesamiento paralelo} de los
procesadores multicore. La Ley de Moore se sigue cumpliendo, sólo que
ahora los transistores se distribuyen en varios núcleos en lugar de
concentrarse en un único procesador.

\subsubsection{Microprocesador Intel I7}\label{microprocesador-intel-i7}

El microprocesador I7 es actualmente el procesador más avanzado para
computadoras personales de la firma Intel. Es retrocompatible con toda
la línea de procesadores de la \textbf{arquitectura x86} desarrollada
por esa firma. La cronología siguiente muestra algunos significativos
cambios de microarquitectura.

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item
  1979: 8086, 8088, primeros microprocesadores de arquitectura x86
\item
  1980: 8087, \textbf{coprocesador numérico}
\item
  1982: 80286, multitarea, \textbf{modo protegido}
\item
  1985: 80386, procesador \textbf{de 32 bits}
\item
  1989: 80486, coprocesador numérico en el mismo circuito integrado
\item
  1993: Pentium, procesador \textbf{superescalar}
\item
  1995: Pentium Pro, procesador con \textbf{ejecución fuera de orden} y
  \textbf{ejecución especulativa}
\item
  1997: Pentium II, procesador que incorpora \textbf{instrucciones
  vectoriales MMX}
\item
  1999: Pentium III, incorpora \textbf{instrucciones vectoriales SSE}
\item
  2000: Pentium IV, mejoras en las instrucciones SSE
\item
  2006: Core 2, nueva microarquitectura, reducción del consumo,
  múltiples núcleos, \textbf{virtualización} en hardware, menores
  velocidades de reloj
\item
  2010: Core i3, i5, i7, procesadores con varias microarquitecturas en
  evolución; presentan desde 2 hasta 12 núcleos, velocidad de reloj
  variable
\end{itemize}

El i7 pertenece a una generación de procesadores donde, para enfrentar
los problemas derivados de la microminiaturización, los diseñadores
optaron por \textbf{replicar}, es decir, incorporar múltiples instancias
de, las unidades de cómputo o \textbf{núcleos}.

\begin{itemize}
\item
  Cada uno de los núcleos, a su vez, puede ejecutar dos secuencias de
  programa independientes (dos \textbf{threads} o \textbf{hilos}).
\item
  Cada núcleo tiene su memoria cache privada, dividida en cache de datos
  y de instrucciones, y además existe un segundo nivel de cache privada
  para datos e instrucciones a la vez.
\item
  Además existe un tercer nivel de memoria cache compartida, donde se
  ubican datos que pueden ser necesitados por cualquiera de los núcleos.
\item
  El procesador integra unidades para controlar la consistencia de la
  memoria interna, y para regular las diferentes actividades dentro del
  microchip a fin de mantener controlados el uso de energía y la
  generación de calor.
\item
  En la misma ``pastilla'' o unidad física del microprocesador se
  encuentra una unidad procesadora de gráficos o GPU. Esta GPU es un
  procesador con una arquitectura especial, dedicado a la generación de
  gráficos avanzados, pero que además puede utilizarse para cómputos
  paralelos de propósito general.
\end{itemize}

\subsection{Tiempos de acceso a los
datos}\label{tiempos-de-acceso-a-los-datos}

Como sabemos, no podemos utilizar un dato si no lo hacemos llegar
primero al procesador o CPU; y el tiempo que tarda en llegar a un
registro de la CPU, para poder operar sobre él, depende de dónde esté
localizado este dato. Es interesante comparar los diferentes tiempos de
demora en el acceso a un dato, o \textbf{latencia}, según en qué
componente del sistema de cómputo se encuentra ese dato.

En la tabla adjunta tomamos como referencia un \textbf{ciclo de CPU}, es
decir, el cambio de estado más pequeño posible en el circuito secuencial
que implementa el procesador de la computadora. Los procesadores
actuales utilizan pulsos de reloj de alrededor de 3 GHz, es decir, el
reloj del sistema genera alrededor de 3.000.000.000 de señales por
segundo; lo que da un tiempo de ciclo de unos 0.3 ns. Una instrucción de
CPU puede llevar uno o varios ciclos para completarse, según la
microarquitectura de la CPU. Pero la CPU, para ejecutar esa instrucción,
necesita tener a disposición, en sus registros, los datos sobre los
cuales debe operar.

\begin{longtable}[c]{@{}lll@{}}
\toprule\addlinespace
Evento & Latencia & Escalado
\\\addlinespace
\midrule\endhead
Un ciclo de CPU & 0.3 ns & 1 s
\\\addlinespace
Acceso a memoria cache & 13 ns & 40 s
\\\addlinespace
Acceso a memoria RAM & 120 ns & 6 min
\\\addlinespace
Disco de estado sólido & 150 $\mu$s & 6 días
\\\addlinespace
Disco magnético & 10 ms & 12 meses
\\\addlinespace
Internet América $\longleftrightarrow$ Europa & 81 ms & 8 años
\\\addlinespace
Reboot & 5 min & 32000 años
\\\addlinespace
\bottomrule
\end{longtable}

¿Cuánto lleva entonces acceder a un dato, en términos de esta duración
básica de un ciclo? Si el dato está en memoria RAM, llevará unos 120 ns
(unos cuatrocientos ciclos). Si está en el disco magnético, demorará
unos 10 ms en llegar al procesador (unos 33000 ciclos).

Entonces, si una instrucción de CPU puede completarse en unos pocos
ciclos, pero debe esperar ¡cientos o miles de ciclos! a que los datos
atraviesen el sistema de memoria, habrá una \textbf{enorme} espera
improductiva para la CPU. Peor aún, si el dato ¡debe llegar desde otro
continente vía la Internet!

Si la CPU tuviera que esperar por los demás componentes, su
\textbf{utilización} se reduciría ridículamente, y su gran velocidad de
procesamiento quedaría completamente desperdiciada. Por esto es que se
establece una \textbf{jerarquía de memoria}, con lugares de
almacenamiento cuyas velocidades de acceso son cada vez mayores a medida
que nos acercamos a la CPU en el sistema de cómputo. Por ejemplo, un
dato que se encuentre en \textbf{memoria cache} (``cerca'' de la CPU)
tendrá una latencia de acceso mucho menor y será preferible a tener que
accederlo desde la memoria RAM (más ``lejana'' en el camino de los
datos).

Hay muchas otras medidas técnicas que toman los ingenieros de las
modernas computadoras para resolver esta disparidad de los tiempos de
acceso, tales como fabricar CPUs con varias unidades de cómputo
(``\textbf{cores} o \textbf{núcleos}'') que funcionan en paralelo, o
diseñar complejos mecanismos de cómputo que reordenan y procesan varias
instrucciones a la vez, de manera de ocultar esas grandes latencias de
acceso.

Para comprender mejor, desde nuestra perspectiva de humanos, la
importancia relativa de esos tiempos de respuesta, la tabla se
\textbf{escala} al tiempo del ciclo de CPU. Es decir, los tiempos bajo
la columna ``Escalado'' son aquellos que tardaría cada acceso \textbf{si
un ciclo de CPU durara un segundo}.

Para completar la tabla, comparamos un ciclo de CPU con el proceso de
\textbf{reboot} o reencendido de la computadora (``¿probó apagar y
volver a encender el equipo?'').

\end{document}
